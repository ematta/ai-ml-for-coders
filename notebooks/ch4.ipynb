{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 09:26:43.907205: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"INTERNAL: Couldn't parse JSON response from OAuth server.\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /Users/ematta/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/external/github.com/ematta/ai-ml-for-coders/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.36 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.33 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.29 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.27 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.26 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.24 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.33 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.26 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.24 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.35 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.19 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.03s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.12s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.12s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.33 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:03<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:04<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:07<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:09<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:10<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:11<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:13<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:14<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:15<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:17<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:18<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:19<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:20<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:21<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:23<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:27<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:32<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:35<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:37<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:38<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:40<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:42<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:45<00:00,  1.33 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:48<00:00,  1.33 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:49<00:00, 15.67s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:49<00:00, 15.67s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:49<00:00, 15.67s/ url]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:49<00:00, 12.36s/ file]\n",
      "Dl Size...: 100%|██████████| 29/29 [00:49<00:00,  1.70s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:49<00:00, 12.36s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset fashion_mnist downloaded and prepared to /Users/ematta/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "mnist_data = tfds.load('fashion_mnist')\n",
    "for item in mnist_data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "mnist_data = tfds.load('fashion_mnist', split='train')\n",
    "assert isinstance(mnist_data, tf.data.Dataset)\n",
    "print(type(mnist_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['image', 'label'])\n",
      "tf.Tensor(\n",
      "[[[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 77]\n",
      "  [227]\n",
      "  [227]\n",
      "  [208]\n",
      "  [210]\n",
      "  [225]\n",
      "  [216]\n",
      "  [ 85]\n",
      "  [ 32]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 61]\n",
      "  [100]\n",
      "  [ 97]\n",
      "  [ 80]\n",
      "  [ 57]\n",
      "  [117]\n",
      "  [227]\n",
      "  [238]\n",
      "  [115]\n",
      "  [ 49]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 81]\n",
      "  [105]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 64]\n",
      "  [ 44]\n",
      "  [ 21]\n",
      "  [ 13]\n",
      "  [ 44]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [114]\n",
      "  [ 80]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 68]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 77]\n",
      "  [108]\n",
      "  [ 34]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 71]\n",
      "  [ 71]\n",
      "  [ 77]\n",
      "  [ 69]\n",
      "  [ 66]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 94]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 63]\n",
      "  [ 95]\n",
      "  [ 66]\n",
      "  [ 68]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 77]\n",
      "  [106]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [128]\n",
      "  [ 64]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 88]\n",
      "  [120]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 83]\n",
      "  [ 83]\n",
      "  [ 66]\n",
      "  [111]\n",
      "  [123]\n",
      "  [ 78]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 85]\n",
      "  [134]\n",
      "  [ 74]\n",
      "  [ 85]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 81]\n",
      "  [ 75]\n",
      "  [ 61]\n",
      "  [151]\n",
      "  [115]\n",
      "  [ 91]\n",
      "  [ 12]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 85]\n",
      "  [153]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 61]\n",
      "  [162]\n",
      "  [122]\n",
      "  [ 78]\n",
      "  [  6]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 30]\n",
      "  [ 75]\n",
      "  [154]\n",
      "  [ 85]\n",
      "  [ 80]\n",
      "  [ 71]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 49]\n",
      "  [191]\n",
      "  [132]\n",
      "  [ 72]\n",
      "  [ 15]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 58]\n",
      "  [ 66]\n",
      "  [174]\n",
      "  [115]\n",
      "  [ 66]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 78]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 66]\n",
      "  [ 49]\n",
      "  [222]\n",
      "  [131]\n",
      "  [ 77]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 69]\n",
      "  [ 55]\n",
      "  [179]\n",
      "  [139]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [ 64]\n",
      "  [ 55]\n",
      "  [242]\n",
      "  [111]\n",
      "  [ 95]\n",
      "  [ 44]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 57]\n",
      "  [159]\n",
      "  [180]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 64]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 55]\n",
      "  [ 66]\n",
      "  [255]\n",
      "  [ 97]\n",
      "  [108]\n",
      "  [ 49]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 66]\n",
      "  [145]\n",
      "  [153]\n",
      "  [ 72]\n",
      "  [ 83]\n",
      "  [ 58]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 80]\n",
      "  [ 30]\n",
      "  [132]\n",
      "  [255]\n",
      "  [ 37]\n",
      "  [122]\n",
      "  [ 60]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [142]\n",
      "  [180]\n",
      "  [142]\n",
      "  [ 57]\n",
      "  [ 64]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [ 21]\n",
      "  [185]\n",
      "  [227]\n",
      "  [ 37]\n",
      "  [143]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 71]\n",
      "  [136]\n",
      "  [194]\n",
      "  [126]\n",
      "  [ 46]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 38]\n",
      "  [139]\n",
      "  [185]\n",
      "  [ 60]\n",
      "  [151]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  4]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [145]\n",
      "  [177]\n",
      "  [ 78]\n",
      "  [ 49]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 72]\n",
      "  [ 63]\n",
      "  [ 80]\n",
      "  [156]\n",
      "  [117]\n",
      "  [153]\n",
      "  [ 55]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [157]\n",
      "  [163]\n",
      "  [ 61]\n",
      "  [ 55]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 60]\n",
      "  [ 98]\n",
      "  [156]\n",
      "  [132]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 13]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [157]\n",
      "  [143]\n",
      "  [ 43]\n",
      "  [ 61]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 58]\n",
      "  [ 80]\n",
      "  [157]\n",
      "  [120]\n",
      "  [ 66]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [156]\n",
      "  [114]\n",
      "  [ 35]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 66]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 63]\n",
      "  [165]\n",
      "  [119]\n",
      "  [ 68]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 23]\n",
      "  [ 85]\n",
      "  [ 81]\n",
      "  [177]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 37]\n",
      "  [173]\n",
      "  [ 95]\n",
      "  [ 72]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 81]\n",
      "  [ 86]\n",
      "  [160]\n",
      "  [ 20]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 89]\n",
      "  [ 78]\n",
      "  [ 81]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 74]\n",
      "  [ 20]\n",
      "  [177]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 49]\n",
      "  [ 77]\n",
      "  [ 91]\n",
      "  [200]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 95]\n",
      "  [ 86]\n",
      "  [ 88]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 83]\n",
      "  [ 89]\n",
      "  [ 86]\n",
      "  [  0]\n",
      "  [191]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 24]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [ 71]\n",
      "  [108]\n",
      "  [165]\n",
      "  [  0]\n",
      "  [ 24]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 57]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 63]\n",
      "  [ 63]\n",
      "  [ 77]\n",
      "  [ 89]\n",
      "  [ 52]\n",
      "  [  0]\n",
      "  [211]\n",
      "  [ 97]\n",
      "  [ 77]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 68]\n",
      "  [ 91]\n",
      "  [117]\n",
      "  [137]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [216]\n",
      "  [ 94]\n",
      "  [ 97]\n",
      "  [ 57]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [115]\n",
      "  [105]\n",
      "  [185]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  1]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [153]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 61]\n",
      "  [ 41]\n",
      "  [103]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [106]\n",
      "  [ 47]\n",
      "  [ 69]\n",
      "  [ 23]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]], shape=(28, 28, 1), dtype=uint8)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 09:40:23.237348: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-09 09:40:23.244282: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_data.take(1):\n",
    "    print(type(item))\n",
    "    print(item.keys())\n",
    "    print(item['image'])\n",
    "    print(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='fashion_mnist',\n",
      "    full_name='fashion_mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/zalandoresearch/fashion-mnist',\n",
      "    data_dir='/Users/ematta/tensorflow_datasets/fashion_mnist/3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=Unknown size,\n",
      "    dataset_size=36.42 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n",
      "      author    = {Han Xiao and\n",
      "                   Kashif Rasul and\n",
      "                   Roland Vollgraf},\n",
      "      title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
      "                   Algorithms},\n",
      "      journal   = {CoRR},\n",
      "      volume    = {abs/1708.07747},\n",
      "      year      = {2017},\n",
      "      url       = {http://arxiv.org/abs/1708.07747},\n",
      "      archivePrefix = {arXiv},\n",
      "      eprint    = {1708.07747},\n",
      "      timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
      "      biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
      "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnist_test, info = tfds.load('fashion_mnist', with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_image, training_label), (test_image, test_label) = tfds.as_numpy(\n",
    "    tfds.load(\n",
    "        'fashion_mnist',\n",
    "        split = ['train', 'test'],\n",
    "        batch_size=-1,\n",
    "        as_supervised=True\n",
    "    )\n",
    ")\n",
    "\n",
    "training_image = training_image / 255.0\n",
    "test_image = test_image / 255.0\n",
    "\n",
    "model = K.Sequential([\n",
    "    K.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    K.layers.Dense(128, activation='relu'),\n",
    "    K.layers.Dropout(0.2),\n",
    "    K.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
